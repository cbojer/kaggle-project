---
title: "R Notebook"
output: document_md
---

# Initilization
```{r, message=FALSE, warning=FALSE}
## Clear Environment & Load Packages
rm(list = ls()) ; gc()
library(magrittr)
library(lubridate)
library(data.table)
library(tsfeatures)
library(purrr)
library(furrr)
library(ggfortify)
library(tidyverse)
library(forecast)
library(patchwork)

## Source Functions
source("R/utils.R")
source("R/prepdata.R")
source("R/multiplot.R")

```

# Prepare Data Information
```{r}
data_dirs <- paste0(list.dirs("../Data")[-c(1,2)], "/combined.RDS")
train_dirs <- list.files("./Cache/train_data", full.names = T)
ids <- sapply(data_dirs, get_id)

data_info <- tribble(
  ~id,    ~data_dir,    ~train_dir,    ~group,                      ~target,         ~index,       ~ts_frequency,
  ids[1], data_dirs[1], train_dirs[1], c("store_nbr", "item_nbr"),  "unit_sales",    "date",       c(1),
  ids[2], data_dirs[2], train_dirs[2], c("air_store_id"),           "visitors",      "visit_date", c(1),
  ids[3], data_dirs[3], train_dirs[3], c("store"),                  "sales",         "date",       c(1),
  ids[4], data_dirs[4], train_dirs[4], c("store", "dept"),          "weekly_sales",  "date",       c(1),
  ids[5], data_dirs[5], train_dirs[5], c("store_nbr", "item_nbr"),  "units",         "date",       c(1),
  ids[6], data_dirs[6], train_dirs[6], c("page"),                   "views",         "date",       c(1)
)       

```

# Capture and prepare training data
```{r, eval = FALSE}
# favorita-grocery-sales-forecasting
idx <- 1
path <- data_dirs[idx]
raw_data <- readRDS(path)
original_train <- raw_data$train %>% janitor::clean_names()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(raw_data, original_train) ; gc()
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()

# recruit-restaurant-visitor-forecasting
idx <- 2
path <- data_dirs[idx]
raw_data <- readRDS(path)
original_train <- raw_data$air_visit_data %>% janitor::clean_names()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(raw_data, original_train) ; gc()
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()

# rossmann-store-sales
idx <- 3
path <- data_dirs[idx]
raw_data <- readRDS(path)
original_train <- raw_data$train %>% janitor::clean_names()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(raw_data, original_train) ; gc()
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()

# walmart-recruiting-store-sales-forecasting
idx <- 4
path <- data_dirs[idx]
raw_data <- readRDS(path)
original_train <- raw_data$train %>% janitor::clean_names()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(raw_data, original_train) ; gc()
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()

# walmart-storm-weather-competition
idx <- 5
path <- data_dirs[idx]
raw_data <- readRDS(path)
original_train <- raw_data$train %>% janitor::clean_names()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(raw_data, original_train) ; gc()
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()

# web-traffic-time-series-forecasting
idx <- 6
path <- data_dirs[idx]
raw_data <- readRDS(path)
train1 <- melt(raw_data$train_1, id.vars = "Page", variable.name = "date", variable.factor = F, na.rm = TRUE, value.name = "views", verbose = T)
train2 <- melt(raw_data$train_2, id.vars = "Page", variable.name = "date", variable.factor = F, na.rm = TRUE, value.name = "views", verbose = T)
original_train <- rbind(train1, train2) %>% janitor::clean_names()
rm(train1, train2, raw_data) ; gc()
train <- as_dtts(original_train, group = data_info$group[[idx]], target = data_info$target[[idx]], index = data_info$index[[idx]], "train")
rm(original_train) ; gc()
train[, views := as.integer(views)]
train[, page := as.factor(page)]
train[, c(attr(train, "self")$index) := parVec(x = get(attr(train, "self")$index), FUN = anytime::anydate)]
save_path <- paste0("Cache/train_data/", get_id(path), ".RDS")
saveRDS(train, save_path)
rm(train) ; gc()
```

# Fill in missing Indexes in Kaggle Data
```{r}
for(idx in 1:nrow(data_info)) {
  print(sprintf("Filling Index in %s", data_info$id[[idx]]))
  
  # Pull Information
  path <- data_info$train_dir[[idx]]
  target <- data_info$target[[idx]]
  group <- data_info$group[[idx]]
  index <- data_info$index[[idx]]
  interval <- ifelse(grepl("week", target), "1 week", "1 day")
  
  # Load & prepare data
  train <- readRDS(path)
  
  # Fill Index
  index_full <- train[, .(seq(min(get(index)), max(get(index)), interval)), by = eval(group)]
  setnames(index_full, "V1", eval(index))
  train <- train[index_full, on = c(eval(group), eval(index))]
  train[is.na(get(target)), eval(target) := 0]
  
  # Save Filled DT
  saveRDS(train, str_replace(path, "train_data", "filled_train_data"))
  
  rm(train) ; gc()
}
```


# Extract Kang, Hyndman, Smith Features from Kaggle Competitions
```{r, eval = FALSE}
for(idx in 1:nrow(data_info)) {
  print(sprintf("Extracting KHS features from %s", data_info$id[[idx]]))
  
  # Load & prepare data
  path <- str_replace(data_info$train_dir[[idx]], "train_data", "filled_train_data")
  train <- readRDS(path)
  train[is.na(get(data_info$target[[idx]])), c(data_info$target[[idx]]) := 0]
  train[get(data_info$target[[idx]]) < 0, c(data_info$target[[idx]]) := 0]
  
  # Set key and index
  setkeyv(train, data_info$group[[idx]])
  setindexv(train, data_info$index[[idx]])
  
  # Extract features
  tictoc::tic("Collect Time Series Features")
  khs_feats <- khs_ts_features(.data = train,
                               target = data_info$target[[idx]],
                               group = data_info$group[[idx]],
                               index = data_info$index[[idx]],
                               frequency = data_info$ts_frequency[[idx]],
                               parallel = T)
  tictoc::toc()
  
  # Save result
  saveRDS(cbind(id = data_info$id[[idx]], khs_feats), paste0("./Results/khs_features_filled/", data_info$id[[idx]], ".rds"))
  
  # Cleanup
  rm(train, khs_feats) ; gc()
}

```

```{r, eval = FALSE}
# Reconfigure M3 data
library(tscompdata)

M3 <- Mcomp::M3

M3DT <- rbindlist(lapply(M3, function(x) {
  data.table(
    sn = x$sn,
    st = x$st,
    n = x$n,
    h = x$h,
    period = x$period,
    type = x$type,
    description = x$description,
    train = list(x$x),
    test = list(x$xx)
  )
}))

saveRDS(M3DT, "Cache/M3DT.RDS")

# Reconfigure M4 data
library(M4comp2018)

data(M4)

M4DT <- rbindlist(lapply(M4, function(x) {
  data.table(
    st = x$st,
    n = x$n,
    h = x$h,
    period = x$period,
    type = x$type,
    train = list(x$x),
    test = list(x$xx)
  )
}))

saveRDS(M4DT, "Cache/M4DT.RDS")
```

# Extract Kang, Hyndmad, Smith Features from M* Competitions
```{r, eval = FALSE}
# Load data
M3 <- readRDS("Cache/M3DT.RDS")
M4 <- readRDS("Cache/M4DT.RDS")

# Extract KHS Features
library(parallel)
cl <- makeCluster(detectCores())
clusterExport(cl, "get_khs_feats")
M3_feats <- M3[, .(id = "M3", rbindlist(pbapply::pblapply(train, get_khs_feats, cl = cl)))]
M4_feats <- M4[, .(id = "M4", rbindlist(pbapply::pblapply(train, get_khs_feats, cl = cl)))]
stopCluster(cl)

# Season is NA when Frequency == 1, i.e., yearly
M3_feats[is.na(Season), Season := 0]
M4_feats[is.na(Season), Season := 0]

# Save results
saveRDS(M3_feats, "Results/khs_features/M3.rds")
saveRDS(M4_feats, "Results/khs_features/M4.rds")
```

# Explore M* Features
```{r, fig.width=10}
M3_feats <- readRDS("Results/khs_features_filled/M3.rds")
M4_feats <- readRDS("Results/khs_features_filled/M4.rds")

p1 <- prcomp(select(M3_feats, -Period, -id), scale=TRUE)$x %>%
  as_tibble() %>%
  bind_cols(Period=M3_feats$Period) %>%
  ggplot(aes(x=PC1, y=PC2)) +
  geom_point(aes(col=Period)) +
  labs(title = "M3 Features")

p2 <- prcomp(select(M4_feats, -Period, -id), scale=TRUE)$x %>%
  as_tibble() %>%
  bind_cols(Period=M4_feats$Period) %>%
  ggplot(aes(x=PC1, y=PC2)) +
  geom_point(aes(col=Period)) +
  labs(title = "M4 Features")

p1 + p2
```


```{r}
# Load KHS features
feature_files <- list.files("Results/khs_features_filled/", full.names = T)
all_features <- rbindlist(lapply(feature_files, readRDS))

# Adjust ID's
all_features[, id := case_when(id == "favorita-grocery-sales-forecasting" ~ "Corporación Favorita",
                               id == "recruit-restaurant-visitor-forecasting" ~ "Recruit Restaurant",
                               id == "rossmann-store-sales" ~ "Rossmann",
                               id == "walmart-recruiting-store-sales-forecasting" ~ "Walmart",
                               id == "walmart-storm-weather-competition" ~ "Walmart Stormy Weather",
                               id == "web-traffic-time-series-forecasting" ~ "Wikipedia",
                               TRUE ~ id)]

# Investigate % of rows that contain NA's
all_features[, .("% of rows without NA" = scales::percent(sum(complete.cases(.SD))/.N)), id]

# Fill NA's with 0
num_cols <- which(sapply(all_features, is.numeric))
setnafill(all_features, fill = 0, cols = num_cols)

# Sort data by ID
setkey(all_features, "id")

# Adjust frequency for Recruit Restaurant
all_features[id == "Recruit Restaurant", Frequency := 1]
all_features[id == "Recruit Restaurant", Period := as.factor(365)]
all_features[, Period := as.factor(as.character(Period))]

complete(all_features)
all_features[is.na(ACF1), ACF1 := 0]
```

```{r}
# Make PCA
pc_obj <- prcomp(select(all_features, -Period, -id), scale=TRUE)

# Pull PC1 & PC2 for all features and bind ID
plot.data <- pc_obj$x %>% 
  as_tibble() %>%
  bind_cols(id = all_features$id)

# Define x and y plot limits
plot.xlims <- c(min(plot.data$PC1) %>% floor(),
                max(plot.data$PC1) %>% ceiling())

plot.ylims <- c(min(plot.data$PC2) %>% floor(),
                max(plot.data$PC2) %>% ceiling())

# Get Unique IDS and Define Color Scheme
unique_ids <- unique(plot.data$id)
color_scheme <- ggthemes::ptol_pal()(length(unique_ids))
unique_cols <- sapply(unique_ids, function(x) {
  color_scheme[grep(eval(x), unique_ids)][1]
})

# Plot Principal Components for each ID
pc_plots <- sapply(unique_ids, function(x) {
  if( x == "M4") {
    bg.plot.data <- plot.data
  } else {
    bg.plot.data <- plot.data %>% filter(id == "M4")
  }
  
  tmp <- plot.data %>%
    filter(id == eval(x))
  
  ggplot() +
    geom_point(aes(x=PC1, y=PC2), colour = "grey92", data = bg.plot.data) +
    geom_point(aes(x=PC1, y=PC2), colour = unique_cols[x], data = tmp) +
    theme_bw(base_size = 12) +
    theme(axis.title.x = element_blank(), 
          axis.title.y = element_blank(),
          axis.text.x = element_blank(),
          axis.text.y = element_blank(),
          axis.ticks = element_blank(),
          plot.title = element_text(hjust = 0.5)) +
    xlim(plot.xlims) + ylim(plot.ylims) +
    facet_grid(~eval(x))
  
}, USE.NAMES = TRUE, simplify = FALSE)

# Pull PC1 & PC2 Loading Data
loading.data <- pc_obj$rotation %>%
  as_tibble(rownames = "rn")

# Define x and y plot limits
loading.xlims <- c(min(loading.data$PC1*3) %>% floor(),
                   max(loading.data$PC1*3) %>% ceiling())

loading.ylims <- c(min(loading.data$PC2*3) %>% floor(),
                   max(loading.data$PC2*3) %>% ceiling())

# Plot Loadings
pc_plots[["loadings"]] <- loading.data %>%
  ggplot() +
  geom_segment(aes(x = 0, y = 0, xend = PC1*2.5, yend = PC2*2.5),
               arrow = grid::arrow(length = grid::unit(8, "points"), type = "closed"),
               col = "firebrick") +
  geom_text(aes(x = PC1*3, y = PC2*3, label = rn), col = "firebrick") +
  xlim(loading.xlims) + ylim(loading.ylims) +
  theme_bw(base_size = 12) +
  theme(axis.title.x = element_blank(), 
        axis.title.y = element_blank(),
        axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank()) +
  facet_wrap(~"Loadings")

# Define Layout for Plot (Loadings [e] will be placed in the middle)
layout <- 'yabc \n ydef \n yghi \n xxxx'

# Calculate Variance Explained by PC's
var_exp <- pc_obj$sdev^2/sum(pc_obj$sdev^2)
var_exp <- var_exp[c(1,2)]

# Concatenate Variance Explained with 
labs <- paste0(paste0("PC", 1:2), " (", scales::percent(var_exp), ")")
x_lab <- grid::textGrob(labs[1], hjust = 0.5)
y_lab <- grid::textGrob(labs[2], rot = 90, vjust = 0.5)

patchwork <- wrap_plots(a=pc_plots$M3,                    b=pc_plots$M4,                       c=pc_plots$`Corporación Favorita`,
                        d=pc_plots$`Recruit Restaurant`,  e=pc_plots$loadings,                 f=pc_plots$Rossmann,
                        g=pc_plots$Walmart,               h=pc_plots$`Walmart Stormy Weather`, i=pc_plots$Wikipedia,
                        x = x_lab, y = y_lab) +
  plot_layout(ncol = 4, nrow = 4, widths = c(0.1, 1, 1, 1), heights = c(1, 1, 1, 0.1), design = layout)

ggsave("plots/khs_features_filled5.png", patchwork, width = 12, height = 10)

patchwork
```



